\documentclass[11]{article}

\usepackage[utf8]{inputenc}

\usepackage{mathpazo}
\usepackage{amssymb,amsthm}
\usepackage[fleqn]{amsmath}
\usepackage[svgnames]{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[breakable,theorems,skins]{tcolorbox}
% \usepackage{multirow}
\usepackage[all]{xy}

%lengths & spacing
\allowdisplaybreaks
\unitlength 1cm
\textheight 22cm
\textwidth 17cm
\oddsidemargin -0.5cm
\evensidemargin -0.5cm
\topmargin -1.5cm
\topskip 0cm
\headheight 0.5cm
\headsep 1cm
%\marginparwidth 1.2cm
\newlength\doubleind
\addtolength{\doubleind}{\leftmargini}
\addtolength{\doubleind}{\leftmarginii}
\parindent 0pt
\def\lstsp{\hspace{\labelsep}}

\def\exstart{\hangindent\leftmargini\textup{1.}\hspace{\labelsep}}

\newcommand\boldinline[1]{\paragraph{#1}}
\newcommand\boldsubsection[1]{\subsection*{#1}}
\newcommand\boldsubsubsection[1]{\subsubsection*{#1}}


                      
\newenvironment{enumeratea}{
	\begin{enumerate}
	  \renewcommand{\labelenumi}{(\alph{enumi})}}
	{\end{enumerate}}


%Theorems
\tcbset{
	defstyle/.style={enhanced, top=3pt, bottom=3pt, colframe=black, coltitle=black, arc=5pt, boxrule=1.5pt, left*=0pt, right*=0pt, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, colback=blue!7!white, grow sidewards by=8pt, drop fuzzy shadow},
	exstyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{orange!75!white}},
	thmstyle/.style={enhanced, top=3pt, bottom=3pt, colframe=black, coltitle=black, arc=5pt, boxrule=1.5pt,left*=0pt, right*=0pt, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\slshape, colback=green!12!white, grow sidewards by=8pt, drop fuzzy shadow},
	exercisestyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercises\ \ \ }},
	proofstyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, blanker, borderline west={4pt}{-8pt}{green!50!white}},
	asidestyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, blanker, borderline west={4pt}{-8pt}{black!50!white}},
	exercisestyle2/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercises\ \thesubsection.\ \ \ }},
	exercisestyle4/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercises\ \thesection.\ \ \ }},
	exercisestyle3/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercise\quad}},
	conjstyle/.style={enhanced, top=3pt, bottom=3pt, colframe=black, coltitle=black, arc=5pt, boxrule=1.5pt,left*=0pt, right*=0pt, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\slshape, colback=red!12!white, grow sidewards by=8pt, drop fuzzy shadow},
	}

\tcolorboxenvironment{proof}{breakable,proofstyle}

\newtcbtheorem[number within=section]{defn}{Definition}{defstyle}{defn}
\newtcbtheorem[use counter from=defn]{example}{Example}{exstyle}{ex}
\newtcbtheorem[use counter from=defn]{examples}{Examples}{exstyle}{ex}
\newtcbtheorem[use counter from=defn]{thm}{Theorem}{thmstyle}{thm}
\newtcbtheorem[use counter from=defn]{lemm}{Lemma}{thmstyle}{lemm}
\newtcbtheorem[use counter from=defn]{cor}{Corollary}{thmstyle}{cor}
\newtcbtheorem[use counter from=defn]{axiom}{Axiom}{defstyle}{axiom}
\newtcbtheorem[use counter from=defn]{axioms}{Axioms}{defstyle}{axioms}
\newtcbtheorem[use counter from=defn]{conj}{Conjecture}{conjstyle}{conj}

\newtcolorbox{aside}{asidestyle}
\newtcolorbox{exercises*}{exercisestyle}
\newtcolorbox{exercises}{exercisestyle2}
\newtcolorbox{exercisessec}{exercisestyle4}
\newtcolorbox{exercise}{exercisestyle3}

\begin{document}

\section{Decision Trees and Ensemble Methods}
\begin{flushleft}
We have looked at various supervised learning models$-$kNN, linear/logistic models, Naive Bayes, LDA/QDA, Neural Networks$-$each with their own spin on classification and regression problems. Now, we consider an approach that has a similar concept to kNN. Consider the usual setup: given a set of data $\lbrace (x_i, y_i) \rbrace$, can we predict where $y$ falls on new data?\\
\subsection{Decision Trees}
This time, we will predict $y$ based on nested \textit{if-else} statements or \textit{claim}. Or, more plainly, an observation will follow a trajectory based on whether it is lower than or above certain numerical quantities. For instance, let's say we want to predict whether a student gets admitted into grad school. We can classify criteria accordingly:\\
\vspace{2mm}
\begin{itemize}
\item{Primary claim: Move left if the student's undergraduate GPA is $\leq 3.5$ (1), and right if it is $>3.5$ (2).}
\item{Secondary claim: Given (1) is true, move left is the student has at most 2 year of work experience (3), and right if they have more than 2 year (4). Given (2) is true, move left if the student has at most 1 year of work experience (5), and right if they have more than 1 year (6).}
\item{Tertiary claim: For students with an undergraduate GPA $\leq 3.5$ and at most 2 years of work experience (3), move left if they participated in no clubs (7) and move right if they participated in at least one club (8).}
\end{itemize}
We can structure our findings as such:\\
\end{flushleft}
\begin{center}

\end{center}
\begin{flushleft}
The resulting diagram is what we call a \textit{decision tree}. It uses a sequence of decisions to partition the $x$-space into $M$ regions$-R_1, R_2, \cdot \cdot \cdot , R_M$. Each nodes $N_i$ are associated with a region $R_i '$ in said $x$-space alongside a decision $D_i$, except ones at the end. We define these nodes \textit{leaves}, or \textit{terminal nodes}, as those without descendants or further decisions/partitions.
\end{flushleft}
\begin{center}
(make minipage)
\end{center}
\begin{flushleft}
Taking the decision tree to the right, $N_3, N_4, N_6, N_7$ are leaves. Say it satisfies the claims $D_1$ and $D_5$ but does not satisfy $D_2$. Then the region $R_6$ is the set $\lbrace x: D_1, \overline{D}_2, D_5 \rbrace$. In general, this decision tree models a \textit{classification} problem, where the response is a binary outcome (accepted to grad school = 1, not accepted = 0). The terminal nodes in the model $(4), (5), (6), (7), (8)$ are the five different regions, or categories of students.\\
\begin{itemize}
\item{(4): Student has GPA $\leq 3.5$ and has more than 2 years of work experience.}
\item{(5): Student has GPA $> 3.5$ and has at most one year of work experience.}
\item{(6): Student has GPA $> 3.5$ and has more than one year of work experience.}
\item{(7): Student has GPA $\leq 3.5$, has at most 2 years of work experience, and has not participated in clubs.}
\item{(8): Student has GPA $\leq 3.5$, has at most 2 years of work experience, and has participated in at least one club.}
\end{itemize}
Such a classification problem could be a (over)simplified heuristic for an admissions board in accepting/denying students, saying that students who fall in the regions $(4): R_1, (6): R_3, $ and $(8): R_5$ should get admitted and denying $(5): R_2$, $(7): R_4$. Here we are just saying that students who fall into node 4 will be grouped into the region $R_1$.\\
\vspace{2mm}
To further simplify the problem, suppose we no longer want to use club participation as a predictor and only use GPA and work experience in our model. How can we draw each region in the $x$-plane?\\
\vspace{2mm}
Suppose $x^{1}$ measures GPA and $x^{(2)}$ measures work experience. Then we are concerned with the claims $x^{(1)} \leq 3.5$ and $x^{(2)} \leq 2$ if the first claim is true and $x^{(2)}$ if false.\\
\end{flushleft}
\begin{center}

\end{center}
\begin{flushleft}
Seeing how the plane is partitioned, an admissions board would be more likely to accept students in \textcolor{blue}{region 2} and \textcolor{purple}{region 4}. Obviously, we cannot guarantee high accuracy, especially if we are only using 2 predictors. If we decided that club participation is significant enough, we would have a 3-dimensional space and $x^{(3)}$ is the predictor associated with clubs.\\
\vspace{2mm}
We now look at examples of different decision tree boundaries.
\begin{example}{}{db1}
Suppose $x^{(1)}$ and $x^{(2)}$ are two predictors and $t_1, t_2, t_3, t_4$ are parameters. Let us define regions $R_1, \cdot \cdot \cdot ,R_5$ by:
\begin{itemize}
\item{$R_1: x^{(1)} < t_1, x^{(2)} < t_2$}
\item{$R_2: x^{(1)} < t_1, x^{(2)} \geq t_2$}
\item{$R_3: x^{(1)} \geq t_1, x^{(2)} < t_3$}
\item{$R_4: x^{(1)} \geq t_1, x^{(1)} \geq t_3, x^{(2)} < t_4$}
\item{$R_5: x^{(1)} \geq t_1, x^{(1)} \geq t_3, x^{(2)} \geq t_4$}
\end{itemize}
Draw the corresponding decision tree and regions in the $x$-plane.
\end{example}
\begin{example}{}{db2}
This time, change $R_3, R_4, R_5$ such that
\begin{itemize}
\item{$R_3: x^{(1)} \geq t_1, x^{(1)} < t_3$}
\item{$R_4: x^{(1)} \geq t_1, x^{(1)} \geq t_3, x^{(2)} < t_4$}
\item{$R_5: x^{(1)} \geq t_1, x^{(1)} \geq t_3, x^{(2)} \geq t_4$}
\end{itemize}
What is the new decision tree and region distribution in the $x$-plane?
\end{example}
In theory, region can have any shape, but we use rectangles for simplicity and ease of interpretation.\\
\vspace{2mm}
The general procedure for generating decision trees and boundaries follow: 
\begin{enumerate}
\item{Divide the predictor space into $J$ distinct and nonoverlapping regions $R_1, ..., R_J$.}
\item{For every observation that falls into the same region, we make the same prediction\\
Regression $-$ Average the training observations in that region\\
Classification $-$ Majority vote of the training observations in that region}
\item{Find $R_1, \cdot \cdot \cdot, R_J$ that minimize the residual sum of squared error (RSS)
$$\text{RSS} = \sum_{j = 1}^J \sum_{i \in R_j} \left(y_i - \overline{y}_{R_j} \right)^2 $$
where $\overline{y}_{R_j}$ is the mean response for the region $R_j$.}
\end{enumerate}
In more complex cases, it may not be feasible to check all possible combinations of regions. So, we may just optimize for one split. At the splitting step, pick a random predictor $X_j$ and a cutoff point $S$, and we split into two groups
$$R_1 ^{(j, s)} = \lbrace x | x_j < s \rbrace , \hspace{5mm} R_i^{(j, s)} = \lbrace x | x_j \geq s \rbrace$$
Then, we minimize
$$\sum_{i: x_i \in R_ (j, s)} (y_i - \hat{y} R_i )^2 + \sum_{i: x_i \in R_2 (j, s)} (y_i - \hat{y}R_2 )^2$$
This is called \textit{recursive learning splitting}.
\end{flushleft}
\end{document}
