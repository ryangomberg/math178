\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[fleqn]{amsmath}
\usepackage[svgnames]{xcolor}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{lipsum}    
\usepackage{nicematrix}
\usepackage[dvipsnames]{xcolor}
\usepackage[breakable,theorems,skins]{tcolorbox}
\usepackage{enumitem}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{multirow}
\usepackage[all]{xy}
\usepackage{tzplot}
\usepackage{caption}
\usepackage{pgfplots}
\usepackage{chngcntr}  
\usepackage{placeins}

\definecolor{purp}{HTML}{961CEB}

\def\exstart{\hangindent\leftmargini\textup{1.}\hspace{\labelsep}}

\newcommand\boldinline[1]{\paragraph{#1}}
\newcommand\boldsubsection[1]{\subsection*{#1}}
\newcommand\boldsubsubsection[1]{\subsubsection*{#1}}
 
\newenvironment{enumeratea}{
	\begin{enumerate}
	  \renewcommand{\labelenumi}{(\alph{enumi})}}
	{\end{enumerate}}


%Theorems
\tcbset{
	defstyle/.style={enhanced, top=3pt, bottom=3pt, colframe=black, coltitle=black, arc=5pt, boxrule=1.5pt, left*=0pt, right*=0pt, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, colback=blue!7!white, grow sidewards by=8pt, drop fuzzy shadow},
	exstyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{orange!75!white}},
	thmstyle/.style={enhanced, top=3pt, bottom=3pt, colframe=black, coltitle=black, arc=5pt, boxrule=1.5pt,left*=0pt, right*=0pt, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\slshape, colback=green!12!white, grow sidewards by=8pt, drop fuzzy shadow},
	exercisestyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercises\ \ \ }},
	proofstyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, blanker, borderline west={4pt}{-8pt}{green!50!white}},
	asidestyle/.style={enhanced, breakable, beforeafter skip balanced=10pt, blanker, borderline west={4pt}{-8pt}{black!50!white}},
	exercisestyle2/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercises\ \thesubsection.\ \ \ }},
	exercisestyle4/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercises\ \thesection.\ \ \ }},
	exercisestyle3/.style={enhanced, breakable, beforeafter skip balanced=10pt, coltitle=black, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\upshape, blanker, borderline west={4pt}{-8pt}{purple!75!white}, title={Exercise\quad}},
	conjstyle/.style={enhanced, top=3pt, bottom=3pt, colframe=black, coltitle=black, arc=5pt, boxrule=1.5pt,left*=0pt, right*=0pt, theorem style=plain, terminator sign={.\ \ \ }, fonttitle=\bfseries\upshape, fontupper=\slshape, colback=red!12!white, grow sidewards by=8pt, drop fuzzy shadow},
	}

\tcolorboxenvironment{proof}{breakable,proofstyle}

\newtcbtheorem[number within=section]{defn}{Definition}{defstyle}{defn}
\newtcbtheorem[use counter from=defn]{example}{Example}{exstyle}{ex}
\newtcbtheorem[use counter from=defn]{examples}{Examples}{exstyle}{ex}
\newtcbtheorem[use counter from=defn]{thm}{Theorem}{thmstyle}{thm}
\newtcbtheorem[use counter from=defn]{lemm}{Lemma}{thmstyle}{lemm}
\newtcbtheorem[use counter from=defn]{cor}{Corollary}{thmstyle}{cor}
\newtcbtheorem[use counter from=defn]{axiom}{Axiom}{defstyle}{axiom}
\newtcbtheorem[use counter from=defn]{axioms}{Axioms}{defstyle}{axioms}
\newtcbtheorem[use counter from=defn]{conj}{Conjecture}{conjstyle}{conj}

\newtcolorbox{aside}{asidestyle}
\newtcolorbox{exercises*}{exercisestyle}
\newtcolorbox{exercises}{exercisestyle2}
\newtcolorbox{exercisessec}{exercisestyle4}
\newtcolorbox{exercise}{exercisestyle3}

\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,calc,patterns}
\usepackage{newpxtext,newpxmath}
\graphicspath{ {./Math 176/} }
\begin{document}
\setlist{nolistsep,leftmargin=*}
\pagestyle{fancy}
\fancyhead[L]{Ryan Gomberg}
\fancyhead[C]{Math 178 Notes (Mathematical Machine Learning)}
\fancyhead[R]{Page \thepage \hspace{0.2mm} of 23}

\begin{flushleft}
\section{Model Types and Algorithms}
\textbf{1.1 \hspace{1mm} Supervised vs. Unsupervised Learning}\\
\vspace{2mm}
Supervised learning is often referred to as \textit{learning with training.} Given data $X, y$, we want to fit the mapping between $X$ and $y$ with training data and make predictions with the new $y$ given new $X$ in the test dataset. The ultimate goal is to minimize the distance between $\hat{y}$, the predicted value, and $y$.\\
\vspace{2mm}
Unsupervised learning is \text{learning without training.} This time, there is no response $y$ to be predicted. Instead, we \textit{explore} the pattern of data using methods such as clustering or dimension reduction.\\
\vspace{2mm}
\textbf{1.2 \hspace{1mm} Regression vs. Classification}\\
\vspace{2mm}
In supervised machine learning models, regression is often used with numerical data and classification is used with categorical data (consists of multiple classes or levels).\\
\vspace{2mm}
\textbf{1.3 \hspace{1mm} Parametric vs. Non-parametric models}\\
\vspace{2mm}
\textit{Parametric} machine learning models assume that the function can be modeled in a functional form and follows a procedure to \textit{fit} and \textit{train} the model. In linear regression, the functional form is a linear combination of unknown parameters and our predictors.\\
\vspace{2mm}
\textit{Non-parametric} machine learning algorithms, in contrast, does not explicitly assume a functional form is possible and limits the need for finding parameters. Instead, it uses patterns and trends in existing training data to predict results in the test data. The K-nearest neighbors algorithm is just one example. Test points are compared to a set amount of training data points that are closest, or similar, to it.\\
\vspace{2mm}
\textbf{1.4 \hspace{1mm} Measuring accuracy in machine learning models}\\
\vspace{2mm}
The standard measurement of accuracy is computing the mean squared error (MSE). If $y_i$ and $x_i$ are observations in training data and $\hat{f}$ is the \textit{function} described by the model, then
$$\text{Training MSE} = \frac{1}{n} \sum_{i = 1}^{n} \left(y_i - \hat{f}(x_i)\right)^2.$$
This is the function to be minimized. Note that we write $y_i = \hat{f}(x_i) + \varepsilon_i$, where $\varepsilon_i$ is the error/noise caused by $x_i$. The MSE for the test data can be expressed as
$$\text{Test MSE} = E[\varepsilon^2] + \underbrace{\left(f(x_0) - E_{x_i, \varepsilon}[\hat{f}(x_0)] \right)^2}_{\text{Bias}(\hat{f}(x_0))} + \underbrace{E_{x_i, \varepsilon}\left[\hat{f}(x_0) - E_{x_i, \varepsilon}[\hat{f}(x_0)]\right]^2}_{\text{Var} (\hat{f}(x_0))}.$$
\pagebreak
Here we use properties of expectations from probability theory. The above equation is merely showing that the test MSE is a sum of the bias, variance, and random error (often negligible).
\begin{itemize}
\item{\textbf{Bias} is the error introduced by making assumptions to simplify the learning process. For example, using a linear model to a dataset with a nonlinear relationship is will have high bias, for it is trying to simplify but does not fit the model. This is \textit{underfitting}.}
\item{\textbf{Variance} is the error caused by small fluctuations in the model's test data. A model with high variance will not only capture the patterns in the training data, but the noise as well. This is \textit{overfitting.} High variance models work well with training data but poorly on new test data.}
\end{itemize}
\vspace{2mm}
Combining these two factors is what we know as the \textbf{Bias-Variance Tradeoff}. Increasing the bias of a linear model lowers the variance, and vice versa. Generally, we want to strike a balance between the two factors that will minimize the test MSE.\\
\vspace{2mm}
\textbf{1.5 \hspace{1mm} Flexible vs. Inflexible Models}\\
\vspace{2mm}
\textit{Flexible} models are often used to capture more complex relationships, such as decision trees or high degree polynomial regression. They can readily adapt to changes in data (i.e. new training/test data). Flexible models are more prone to overfitting and consequently having high variance.\\
\vspace{2mm}
\textit{Inflexible} models follow a rigid, functional form that cannot capture complex patterns in data and assume a rigid, predefined relationship between inputs and outputs. Flexible models tend to have more bias and succumb to underfitting.\\
\vspace{2mm}
\textbf{1.6 \hspace{1mm} K-Nearest Neighbors (KNN) Algorithm}\\
\vspace{2mm}
The K-Nearest Neighbors method is a supervised learning algorithm that makes predictions based on how \textit{similar} new data points are to existing data. We compute the Euclidean distance between the test data point and all of the observed data, then choosing $k$ of the existing points that are closest in distance.\\
\vspace{2mm}
\begin{itemize}
\item{In a classification setting, the majority class among the $k$ neighbors determines the predicted class.}
\item{In a regression setting, the predicted value is the average of the values of the $k$ neighbors.}
\end{itemize}
$k$ is what we call a \textit{hyperparamter}, or tuning parameter; it does not depending on the training process. As we increase $k$, the flexibility of the algorithm increases. If we choose too many neighbors, we may observe overfitting and misleading predictions.\\
\vspace{2mm}
\textbf{1.7 \hspace{1mm} Normal Equation for Linear Regression}\\
\vspace{2mm}
Recall that a linear model with $p$ predictors is approximated by
$$\hat{f}(x) = c_0 + c_1 x_1 + c_2 x^2 + \cdot \cdot \cdot c_p x^p.$$
Where $c_0, \cdot \cdot \cdot, c_p$ are unknown parameters. We want to find these parameters in a way that will minimize the error
$$\min_{\overline{w}} \sum_{i = 1}^n y_i - \hat{f}(x_i).$$
$\overline{w}$ is just the set of parameters we want to solve for in the above problem. Recall that with higher degree polynomial regression (degrees of freedom > 2), it is nonlinear with respect to $x$ but it is linear with respect to the parameters. Hence, we can apply methods from linear algebra to solve the optimization problem. The most common approach is through \textit{gradient descent}, for which we take partial derivatives of each parameter and iterate until we find the minimum. The solution is given by the \textbf{normal equation}:
$$\widehat{W} = \left(M^T M\right)^{-1} M^T \overrightarrow{Y}.$$
If $n$ is the number of observations in our training data, then $M$ is a $n \times (p + 1)$ matrix containing the intercept terms (column of 1s) and the predictors in our training data. $\overrightarrow{Y}$ is a $n \times 1$ vector consisting of the response values in our training data.\\
\vspace{5mm}
\textbf{Degrees of freedom}: The number of independent terms used to fit the data. A polynomial of degree $p$ has degree of freedom $= p + 1$ (remember to include the intercept term). It is simply another measurement of flexibility to fit the data. \\
\vspace{5mm}
\textbf{Gradient descent problem}
$$\min_{\overrightarrow{c}} \phi(\overrightarrow{c}) = \min_{\overrightarrow{c}} ||\overrightarrow{y} - X\overrightarrow{c}||^2.$$
\pagebreak
\section{Classification and Generative Models}
Previously, we assumed that we can fit a model $y = f(x) + \varepsilon$, where $\varepsilon$ does not depend on $x$. Now, suppose we let the error also depend on $x$. Then, $y = f(x) + \varepsilon (x)$. We say that $f(x)$ is deterministic and $\varepsilon(x)$ is a random variable. The main objective is to find
$$p(y, x) \approx P(Y = y \hspace{1mm} | \hspace{1mm} X = x)$$
or, that approximation of a model relative to the probability that it predicts $y$ given $x$. For the models in this section, we assume that the $Y$ is a discrete random variable to avoid complications.\\
\vspace{3mm}
\textbf{2.1 \hspace{1mm} Maximum Likelihood Estimation}\\
\vspace{2mm}
We proceed by using a standard logistic regression model. Assume that we are given data $(\overrightarrow{x_i}, y_i)$, where $Y$ can take on two classes: $A$ or $B$. Our goal is to have our model $\hat{p}$ roughly estimate the probability that each outcome occurs. A standard linear regression model would argue that we can approximate in the form $Y = \beta_0 + \beta_1 X$. However, this can quickly violate our goal! Recall that our objective function is a probability function, so the range of values must be $0 < P(Y = y \hspace{1mm} | \hspace{1mm} X = x) < 1$. This is where we introduce logistic regression: a model that takes on the form
$$\hat{p} (Y \in A \hspace{1mm} | \hspace{1mm} X = x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}.$$
This so-called \textit{standardized} form alleviates the negativity issue and forces all outputs to be within 0 and 1. Now that we have developed a probability function, we hit another wall. How can we ensure that we achieve, or get close to, the observations $(\overrightarrow{x_i}, y_i)$ under our new model? The idea is to \textit{maximize} the probability that we get said observations. This is where we introduce the \textbf{likelihood function}:
$$\mathcal{L}\left(\beta_0, \overrightarrow{\beta}\right) = \prod_{i = 1}^n P(Y = y_i \hspace{1mm} | \hspace{1mm} X = x_i) = \begin{cases}
\hat{p}(x) & \text{ when } y \in A\\
1 - \hat{p}(x) & \text{ when } y \in B\\
\end{cases}$$
where $n$ observations are given and assumed that each observation is independent and $\beta_0, \overrightarrow{\beta}$ contains the coefficients $\beta_1, \cdot \cdot \cdot, \beta_n$. We can rewrite this as the product of probabilities that an observation falls into each class:
$$\mathcal{L}\left(\beta_0, \overrightarrow{\beta}\right) = \prod_{i: y_i \in A} \hat{p} (x_i) \prod_{i: y_i \in B} (1 - \hat{p}(x_i)). $$
Finding such $\beta_0, \overrightarrow{\beta}$ that maximizes $\mathcal{L}$ is the \textbf{Maximum Likelihood Estimator (MLE)}.
\pagebreak
\begin{example}{}{MLE}
Suppose we are given a fair die and we roll it four times with the observed outcomes 5, 2, 3, 		6. Let $\theta$ be the probability of rolling a 3. What is the Maximum Likelihood Estimation 		for $\theta$?\\
\vspace{1mm}
Let $P(3) = \theta$ and $P(\text{Not } 3) = 1 - \theta$. Then,
$$f(\theta) = \theta(1 - \theta)^3.$$
We take the natural log of $f$, differentiate, and set equal to zero to find the corresponding $\theta$:
$$\ln (f(\theta)) = \ln (\theta) + 3\ln(1 - \theta) \Longrightarrow \frac{d}{d\theta} \ln (f(\theta)) = \frac{1}{\theta} + \frac{3}{1 - \theta} = 0 \Longleftrightarrow \theta = \frac{1}{4}.$$ 
One could argue that this is indeed the maximum through the second derivative test.
\end{example}
\vspace{3mm}
\textbf{2.2 \hspace{1mm} Maximum A Posteriori (MAP) Estimation}\\
\vspace{2mm}
Recall Bayes' Formula from probability theory. If $A$ and $B$ are both events, each assigned their own probability, then the conditional probability
$$P(A | B) = \frac{P(A) P(B | A)}{P(B)}.$$
Here $P(A | B)$ is the \textit{posterior probability}, or the probability obtained after we take our data into consideration. $P(A)$ is the \textit{prior}, or the probability before we take our data into consideration. If we are given prior knowledge or data, then we can try to \textit{maximize} Bayes' Theorem, or the posterior probability. If $\theta$ is the parameter of interest and $X$ is our observed data, then
$$\underset{\theta}{\text{argmax}} \hspace{1mm} P(X | \theta) P(\theta). $$
This is the \textbf{Maximum A Posteriori Estimation}. Since we are maximizing with respect to $\theta$, we treat $P(X)$ as a constant and can ignore it.\\
\vspace{2mm}
\begin{example}{}{MaxaPost}
	Use the same setup from the previous example. Given the priors 0.8, 0.45, 0.1, 0.04 for $		\theta = $ 0.5, 0.3, 0.7, 0.6 respectively, compute the Maximum A Posteriori Estimation for $\theta$.  
Compute the posteriori for each prior $\theta$ (where $P(X | \theta) = f(\theta)$ from past example) and determine which is the largest:\\
$\theta = 0.5 \Longrightarrow (0.5)^4 (0.8) \approx 0.08$\\
$\theta = 0.3 \Longrightarrow 0.3(0.7)^3 (0.45) \approx 0.0463$\\
$\theta = 0.7 \Longrightarrow 0.7(0.3)^3(0.1) \approx 0.00189$\\
$\theta = 0.6 \Longrightarrow 0.6(0.4)^3 (0.04) \approx 0.001536$\\
The Maximum A Posteriori is therefore 0.08 for when $\theta = 0.05$.
\end{example}
\vspace{2mm}
Similar to MLE, we can compute MAP by taking the natural log and differentiating to easily obtain the maximum.\\
\vspace{2mm}
For logistic models, we choose an objective MLE (not MAP!) function, optimize, and apply new inputs to predict new samples $\hat{p}\left(Y | \overrightarrow{X} = X_0\right)$ directly.\\
\vspace{3mm}
\textbf{2.3 \hspace{1mm} Generative Models: Naive Bayes, LDA, QDA}\\
\vspace{2mm}
Compared to logistic regression, generative models are used to learn how the data itself is distributed through training the data and then testing the model on new data. Hence, they aim to learn the joint probability distribution $P(X, Y)$ (supervised case) or $P(X)$ (unsupervised case). For now, we focus on three generative models: Naive Bayes and Linear/Quadratic Discriminant Analysis.\\
\vspace{2mm}
As given by its name, the Naive Bayes' algorithm is derived from Bayes' formula. For Naive Bayes, the underlying assumption is that the observations in \textit{each class} is independent of each other. Thus, we apply the basic law of probability $P(A \cap B) = P(A)P(B)$. Let $\overrightarrow{X}$ be the vector containing our features and let $\left(X^j | Y = k \right)$ be the $j$-th observation in the $k$-th class. Then, the Naive Bayes' formula is given as
$$\hat{p}\left(\overrightarrow{X} | Y = k \right) = \prod_j \hat{p}\left(X^j \hspace{1mm} | \hspace{1mm} Y = k \right).$$
The expression itself says that the probability that a sample falls into a class is computed as the product of probabilities for each feature within that class. We also say that this conditional probability is directly proportional to the prior of the class. Generally, Naive Bayes is put to quick use for simple classification problems, such as spam/fraud detection, or recommendation systems.\\
\pagebreak
Now, we consider two other algorithms that rely on a different distribution than Naive Bayes. To motivate this idea, we once again consider Bayes' Theorem. Suppose we are given data $\underset{ i = \lbrace 1, \cdot \cdot \cdot, N \rbrace}{\lbrace (x_i, y_i) \rbrace}$ and we want to classify $x_0$ into $k$ classes. Then we compute
$$P[Y = k | X = x] = \frac{P[Y = k]P[X = x | Y = k]}{P[X = x]}.$$
Suppose we cannot assume that the observations in each class are independent. \\
\vspace{1mm}
\begin{itemize}
\item{We know how to compute $P[Y = k]$ within a dataset.\\
\vspace{1mm}}
\item{How do we find $P[X = x | Y = k]?$}
\end{itemize}
\vspace{2mm}
While there is no definitive answer, the easiest (and most used) approach is to make an assumption about the type of distribution that it follows. Let $P[X = x | Y = y] = f_k (x)$ and say that $f_k(x)$ follows a normal Gaussian distribution with mean $\mu_k$ (mean parameter for $k$-th class) and variance $\sigma_k^2$ (variance for $k$-th class). Then, 
$$f_k (x) = \frac{1}{\sqrt{2\pi} \sigma_k} e^{\frac{(x - \mu_k)^2}{2\sigma_k^2}}.$$
We can summarize the behavior of this distribution by writing $f_k (x) \sim \mathcal{N}\left(\mu_k, \sigma_k^2 \right).$\\
\vspace{2mm}
As for $P[X = x]$, we multiply the probability that an observation lies in the $l$-th class (for $1 \leq i \leq k$) by the probability that $X$ takes on a certain outcome $x$ given that it is in the $l$-th class. Then, you sum this across all $k$ classes. Mathematically, this is expressed as
$$P[X = x] = \sum_{l = 1}^{k} P[Y = l] P[X = x | Y = l] \hspace{5mm} \text{(weighted average)}.$$
Before proceeding, let us clean up with some more notation. Let $p_k (x) = P[Y = k | X = x]$ and $\pi_k = P[Y = k]$ (prior). Notice how we can rewrite $P[X = x]$ using normal distributions as well:
$$P[X = x] = \sum_{l = 1}^k \pi_l \cdot \frac{1}{\sqrt{2\pi} \sigma_l} e^{\frac{(x - \mu_l)^2}{2\sigma_l^2}}.$$
This enables us to obtain the final form of $p_k(x)$:
$$p_k(x) = \frac{\pi_k \cdot \frac{1}{\sqrt{2\pi} \sigma_k} e^{\frac{(x - \mu_k)^2}{2\sigma_k^2}}}{\sum_{l = 1}^k \pi_l \cdot \frac{1}{\sqrt{2\pi} \sigma_l} e^{\frac{(x - \mu_l)^2}{2\sigma_l^2}}}.$$
\\
\pagebreak
Now that we have a closed-form expression, we want to optimize it. That is to say, we want to maximize the posterior probability, or probability that we put a sample in the $k$-th class given the features in $X$. Once again, logarithms come to the rescue! Define $\delta_k (x) = \ln (p_k(x))$. Then,
$$\delta_k (x) = -\frac{1}{2\sigma_k} \left(x - \mu_k \right)^2 + \ln(\pi_k) + \ln \left(\frac{1}{\sqrt{2\pi} \sigma_k} \right).$$
We seek to find the $k$ that solves the optimization problem
$$\underset{k \in \lbrace 1, \cdot \cdot \cdot, K \rbrace}{\text{argmax}} \hspace{1mm} \delta_k (x).$$
This algorithm is called \textbf{Quadratic Discriminant Analysis (QDA)}, namely because $\delta_k (x)$ is quadratic in $x$. QDA uses the assumption that the variance for each class is different. If we assume a constant variance across all $K$ classes, then we can eliminate some terms. So, under the assumption $\sigma_1^2 = \sigma_2^2 = \cdot \cdot \cdot = \sigma_k^2 = \sigma^2$:
$$\underset{k \in \lbrace 1, \cdot \cdot \cdot, K \rbrace}{\text{argmax}} \hspace{1mm} -\frac{1}{2\sigma^2} (x - \mu_k)^2 + \ln(\pi_k) = -\frac{x^2}{\sigma^2} + \frac{2x\mu_k}{\sigma^2} - \frac{\mu_k^2}{\sigma^2} + \ln(\pi_k).$$
Note that the first term does not depend on $k$, so we treat it as a constant and drop it. Therefore, we have
$$\underset{k \in \lbrace 1, \cdot \cdot \cdot, K \rbrace}{\text{argmax}} \hspace{1mm} \frac{2x\mu_k}{\sigma^2} - \frac{\mu_k^2}{\sigma^2} + \ln(\pi_k).$$
This form is called \textbf{Linear Discriminant Analysis (LDA)} because it is linear in $x$. Therefore, LDA is a reduced form of QDA if we want to make the simplifying assumption of constant variance.\\
\vspace{2mm}
We can shift perspectives into rewriting both forms using matrices. For simplicity, we omit the derivation and write its closed-form expression. Let $\mu_k$ be the sample mean vector in the $k$-th class, $X$ be the feature vector in class $k$, and $\Sigma_k$ be the covariance matrix of the $k$-th class. Then,
$$\hat{p}\left(X = \overrightarrow{x} | Y = k \right) = \frac{1}{\sqrt{2\pi} |\hat{\Sigma}_k|^{0.5}} e^{-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}_k^{-1} (x - \hat{\mu}_k)}.$$
The closed form optimization function is (at $\overrightarrow{x} = \overrightarrow{x}_0$):
$$\underset{k}{\text{argmax}} \hspace{1mm} \ln(\hat{\pi}_k) - \frac{1}{2} \ln \left|\hat{\Sigma}_k \right| + \frac{1}{2} \overrightarrow{x}_0^T \hat{\Sigma}_k^{-1}\overrightarrow{x}_0 + \overrightarrow{x}_0^T \hat{\Sigma}_k^{-1} \overrightarrow{\mu}_k - \frac{1}{2} \overrightarrow{\mu}_k^T \hat{\Sigma}_k^{-1} \overrightarrow{\mu}_k.$$
Recall that $\overrightarrow{x}_0^T \hat{\Sigma}_k^{-1}\overrightarrow{x}_0$ is a quadratic form, which is therefore the QDA in vector/matrix form. In LDA, the second and third terms will cancel since we assume that the covariance matrix $\Sigma$ is uniform across all classes and so those terms will no longer depend on $k$.\\
Here we think of the covariance matrix as how strong one feature is correlated to another (non-diagonal elements) and the spread of each feature (variance). For QDA we assume that the correlation between features between classes and for LDA we assume that they are the same between classes.\\
\vspace{2mm}
Some closing thoughts about LDA vs. QDA:\\
\vspace{1mm}
\begin{itemize}
\item{If we are concerned about the number of parameters used, then LDA is better, for QDA requires roughly $k$ times the number of parameters compared to LDA.\\
\vspace{1mm}}
\item{QDA is more flexible than LDA. If we think about the assumptions, LDA requires us to assume constant variance whereas QDA does not. As always, flexible models are more likely to overfit test data, so we once again need to determine the true pattern of the data.\\
\vspace{1mm}}
\item{QDA is better than LDA if we have a large sample size and relatively small number of predictors. The approximation for each covariance matrix will smooth out and generally be more precise (lower variance). LDA will have more trouble here because it wants to find a uniform covariance matrix across all classes, which will be harder to estimate with the large sample size. If the true covariance matrices of each class differ significantly, then there will be large bias for LDA's uniform covariance matrix assumption. LDA ultimately forces to treat every class as having the same spread and relationship relative to each other. In low dimensional data, LDA becomes too restrained.}
\end{itemize}
\vspace{2mm}
\textit{Brief Aside}: using Bayes' Theorem and notation $\pi_k, f_k, p_k$, the Gaussian Naive Bayes is written as 
$$p_k (x) = \frac{\pi_k f_{k_1}(x_1) \cdot \cdot \cdot f_{k_p}(x_p)}{\sum_{l = 1}^k \pi_l f_{l_1}(x_1) \cdot \cdot \cdot f_{l_p} (x_p)}.$$
For example, if we wanted to determine the number of parameters required for Naive Bayes, we would need $k$ priors plus $2pk$ since $f_{k_i}$ has $p$ parameters and $k$ classes.\\

\vspace{3mm}
\textbf{2.4 \hspace{1mm} Decision Boundaries}\\
\vspace{2mm}
Simply put, decision boundaries is what really defines classification models; it is how we separate classes from each other. In general, describing decision boundaries for LDA/QDA are straightforward:\\
\vspace{1mm}
\begin{itemize}
\item{The decision boundaries in LDA are linear. That is to say, the boundaries separating each class are straight lines.\\
\vspace{1mm}}
\item{The decision boundaries in QDA are quadratic. That is to say, the boundaries separating each class are parabolic.\\
\vspace{1mm}}
\item{The decision boundaries in Naive Bayes are, in most cases, moderately non-linear.\\
\vspace{1mm}}
\item{The decision boundaries in logistic regression are the same as LDA, but they are computed in different ways.}
\end{itemize}
If the decision boundary is complex (in which it cannot be generated by the above algorithms), then we resort to a non-parametric method. We will look at kNN in particular:
\end{flushleft}
\begin{center}
\includegraphics[width=0.8\textwidth]{kNNboundary}
\end{center}
\begin{flushleft}
Suppose we want to put $\overrightarrow{x}$ into a class using its two nearest neighbors $\overrightarrow{x_1}$ and $\overrightarrow{x_3}.$ We want to find the region that is closest to both neighbors, which in this case is the region shaded in blue. Therefore, we would put $\overrightarrow{x}$ into the blue class over the red class.\\
\vspace{2mm}
The regions are generated by \textit{splitting} the plane in half between pair of observations. While the above example doesn't fully live up to that, the intuition is hopefully there. Another example exhibits the difference of predicted region depending on number of neighbors used (1 or 2).
\end{flushleft}
\begin{center}
\includegraphics[width=0.52\textwidth]{knn2}
\end{center}
\begin{flushleft}

\begin{example}{}{oh}
(To be computed: Decision boundary for LDA)
\end{example}
\pagebreak
\section{Resampling Methods}
The process we have streamlined thus far has been to build a model, train it on existing data (choose objective function and optimize), and then test it on new data. So far, we have assumed that one set of training and test data is sufficient. What if we want, given a fixed amount of data, many sets of train and test data? We refer to this as \textit{resampling}, or generating different samples of training data to get a better understanding of how our models hold up. For example, if we have 100 observations, we could generate 20 sets of (90 training data, 10 test data) models, and see how they compare to each other. Recall from earlier that, given a model $f(x)$ with predicted value and observed value, $f(x_0), y$, 

$$\text{Test Error } = E_{\varepsilon, \text{ train}}\left[y - \hat{f}(x_0)\right]$$

We applied this to a single set of test data. Partitioning our training just once is wasteful; we are not using the available data to its maximum capacity! This is the motivation for resampling methods.\\
\vspace{1mm}
There are two general approaches to resampling:
\vspace{1mm}
\begin{itemize}
\item{Pretend there is no separated test data. Instead, choose different training and test sets, out of the entire data, in each sample.\\
\vspace{1mm}}
\item{Ignore the initial set of test. Within the training data, partition into new subsets of training and test data. Then, validate on the initial set of test data}
\end{itemize}

Validating our data really means that we want to \textit{test} the performance of our model, generated by the subsets of training data, on the subsets of test data before using the ``real" test data. This is a way of using observations as a preliminary test before generalizing a model to unseen data.\\
\vspace{1mm}
In the following sections, we will discuss three methods of resampling, which approach they fall into, and dive into their cost-benefit trade-off.

\subsection{Leave One Out Cross Validation}
Suppose we are given a dataset with $N$ observations. The Leave One Out Cross Validation (LOOCV) method constructs $N$ \textit{folds}, or samples, each containing $N - 1$ training data. The remaining observation is the test data. The accuracy is measured by averaging the performance of the $N$ models.
\begin{center}
\includegraphics[width = 0.5\textwidth]{loocv}
\end{center}

\begin{flushleft}
The blue rectangles indicate the set of data we are training, and the red rectangle is the singular test data. \\
\vspace{1mm}
With the large amount $(N)$ of models generated by LOOCV, the accuracy will be more promising compared to other methods because we are using all $N$ observations in each fold, utilizing the data to its full potential. The model works well in practice; however, the computational cost grows significantly as the number of observations increases. Therefore, this method is typically avoided with large datasets and used for when $N$ is small.\\
\vspace{1mm}

\subsection{K-Fold Cross Validation}
$K$-Fold validation relaxes the restrictions imposed on partitioning data. Instead of generating $N$ models, we partition the training data into $k < N$ groups. For each group, or fold, we train on $k - 1$ groups and test on the remaining group. This is performed $k$ times to ensure each group is the test data one time. \\
\end{flushleft}
\vspace{1mm}
\begin{center}
\includegraphics[width = 0.79\textwidth]{kfold}
\end{center}

\begin{flushleft}
The figure performs a 5-fold cross validation $(k = 5)$. First, we partition our entire dataset into training and test data, which will be kept aside for now. For the time being, we operate on the training data by splitting it into 5 equal folds. In the first iteration, we let \textcolor{blue}{folds 2$-$5} be the training data and then test the new-found model on \textcolor{red}{fold 1}. We iterate four more times until each fold has been used as test data. Then, we compute the average performance across the 5 folds $(\overline{a})$. Then, we generate a model on the entire training set (combining all 5 folds) and then test on the data we set aside earlier.\\
\vspace{2mm}
Typically, to lessen computational cost, $k = 5$ or $k = 10$ are reasonable choices. \\
\vspace{2mm}
$K$-Fold Cross Validation is a great resampling method; it efficiently takes advantage of training data before using any test data. Ultimately, we will get an idea of our model will perform before applying it on new, unseen data.\\

\subsection{Bootstrapping}
Compared to the other two approaches, bootstrapping takes on a more general philosophy. We generate $k$ subsets of our observation data with replacement, each containing $n$ data points, and apply it on unseen data.
\end{flushleft}
\begin{center}
\includegraphics[width = 0.6\textwidth]{bootstrapping}
\end{center}
\begin{flushleft}
Suppose our data has 25 observations. The bootstrapping method applied here takes 3 subsets of observation data, each containing 20 data points. We would then obtain 3 unique models. \\
\vspace{2mm}
Bootstrapping is incredibly powerful in repeated sampling. Of course, we will obtain a higher accuracy with more samples. 3 is way too small; generally 500-1000 of samples are more reasonable with larger data. This, in turn, will increase accuracy but also increase the computational cost. Overall, bootstrapping gives us some intuition on the \textit{variance} between splits and the uncertainty of accuracy.
\end{flushleft}


\pagebreak
\section{Neural Networks, Deep Learning}
1. Overview of Neural Networks, Motivation\\
2. Common Transfer/Activations\\
3. Examples of Boolean NN\\
4. Gradient Descent and Backpropagation 

\pagebreak
\section{Decision Trees and Ensemble Methods}
We have looked at various supervised learning models$-$kNN, linear/logistic models, Naive Bayes, LDA/QDA, Neural Networks$-$each with their own spin on classification and regression problems. Now, we consider an approach that has a similar concept to kNN. Consider the usual setup: given a set of data $\lbrace (x_i, y_i) \rbrace$, can we predict where $y$ falls on new data?\\
\subsection{Decision Trees}
This time, we will predict $y$ based on nested \textit{if-else} statements or \textit{claim}. Or, more plainly, an observation will follow a trajectory based on whether it is lower than or above certain numerical quantities. For instance, let's say we want to predict whether a student gets admitted into grad school. We can classify criteria accordingly:\\
\vspace{2mm}
\begin{itemize}
\item{Primary claim: Move left if the student's undergraduate GPA is $\leq 3.5$ (1), and right if it is $>3.5$ (2).}
\item{Secondary claim: Given (1) is true, move left is the student has at most 2 year of work experience (3), and right if they have more than 2 year (4). Given (2) is true, move left if the student has at most 1 year of work experience (5), and right if they have more than 1 year (6).}
\item{Tertiary claim: For students with an undergraduate GPA $\leq 3.5$ and at most 2 years of work experience (3), move left if they participated in no clubs (7) and move right if they participated in at least one club (8).\\
\vspace{2mm}}
\end{itemize}
We can structure our findings as such:\\
\end{flushleft}
\begin{center}
\includegraphics[width = 0.6\textwidth]{dtree1}
\end{center}
\begin{flushleft}
The resulting diagram is what we call a \textit{decision tree}. It uses a sequence of decisions to partition the $x$-space into $M$ regions$-R_1, R_2, \cdot \cdot \cdot , R_M$. Each nodes $N_i$ are associated with a region $R_i '$ in said $x$-space alongside a decision $D_i$, except ones at the end. We define these nodes \textit{leaves}, or \textit{terminal nodes}, as those without descendants or further decisions/partitions.\\

\begin{minipage}[t]{0.68\linewidth}\vspace{0pt}
Taking the decision tree to the right, $N_3, N_4, N_6, N_7$ are leaves. $D_i$ are the decisions corresponding to the nodes $N_i$. Therefore, there are three decision nodes: $D_1$, $D_2$, and $D_5$ for the nodes $N_1, N_2, N_5$. If an observation fails to satisfy a claim, then we denote it as $\overline{D}_i$. Say an observation satisfies the claims $D_1$ and $D_5$ but does not satisfy $D_2$. Then the region $R_2$ (node $N_6$) is the set $\lbrace x: D_1, \overline{D}_2, D_5 \rbrace$.\\
 
The grad school decision tree models a \textit{classification} problem, where the response is a binary outcome (accepted to grad school = 1, not accepted = 0). The terminal nodes in the model $R_1, R_2, R_3, R_4, R_5$ are the five different regions, or categories of students.\\
\end{minipage}
\begin{minipage}[t]{0.3\linewidth}\vspace{0pt}
\includegraphics[width = 0.9\textwidth]{dtree2}
\end{minipage}
\begin{itemize}
\item{$R_1$: Student has GPA $\leq 3.5$ and has more than 2 years of work experience.}
\item{$R_2$: Student has GPA $> 3.5$ and has at most one year of work experience.}
\item{$R_3$: Student has GPA $> 3.5$ and has more than one year of work experience.}
\item{$R_4$: Student has GPA $\leq 3.5$, has at most 2 years of work experience, and has not participated in clubs.}
\item{$R_5$: Student has GPA $\leq 3.5$, has at most 2 years of work experience, and has participated in at least one club.\\}
\end{itemize}
\vspace{2mm}
Such a classification problem could be a (over)simplified heuristic for an admissions board in accepting/denying students, saying that students who fall in the regions $R_1, R_3, $ and $R_5$ should get admitted and denying $R_2$, $R_4$.\\
\vspace{2mm}
To further simplify the problem, suppose we no longer want to use club participation as a predictor and only use GPA and work experience in our model. How can we draw each region in the $x$-plane?\\
\vspace{2mm}
Suppose $x^{1}$ measures GPA and $x^{(2)}$ measures work experience. Then we are concerned with the claims $x^{(1)} \leq 3.5$ and $x^{(2)} \leq 2$ if the first claim is true and $x^{(2)}$ if false.\\
\vspace{1mm}
(See next page)\\
\pagebreak
\begin{minipage}{0.5\linewidth}\vspace{0pt}
Seeing how the plane is partitioned, an admissions board would be more likely to accept students in \textcolor{blue}{region 2} and \textcolor{purp}{region 4}. Obviously, we cannot guarantee high accuracy, especially if we are only using 2 predictors. If we decided that club participation is significant enough, we would have a 3-dimensional space and $x^{(3)}$ is the predictor associated with clubs.\\
\end{minipage}
\begin{minipage}{0.48\linewidth}\vspace{0pt}
\includegraphics[width = 0.99\textwidth]{dtree3}
\end{minipage}

\vspace{2mm}
We now look at examples of different decision tree boundaries.
\begin{example}{}{db1}
Suppose $x^{(1)}$ and $x^{(2)}$ are two predictors and $t_1, t_2, t_3, t_4$ are parameters. Let us define regions $R_1, \cdot \cdot \cdot ,R_5$ by:
\begin{itemize}
\item{$R_1: x^{(1)} < t_1, x^{(2)} < t_2$}
\item{$R_2: x^{(1)} < t_1, x^{(2)} \geq t_2$}
\item{$R_3: x^{(1)} \geq t_1, x^{(2)} < t_3$}
\item{$R_4: x^{(1)} \geq t_1, x^{(1)} \geq t_3, x^{(2)} < t_4$}
\item{$R_5: x^{(1)} \geq t_1, x^{(1)} \geq t_3, x^{(2)} \geq t_4$}
\end{itemize}
Draw the corresponding decision tree and regions in the $x$-plane.
\begin{figure}[H]
\centering
\includegraphics[width=0.48\textwidth]{dtree6}
\hspace{1mm}
\includegraphics[width=0.48\textwidth]{dtree4}
\end{figure}
\end{example}
\begin{example}{}{db2}
This time, change $R_3, R_4, R_5$ such that
\begin{itemize}
\item{$R_3: x^{(1)} \geq t_1, x^{(2)} < t_4, x^{(1)} < t_3$}
\item{$R_4: x^{(1)} \geq t_1, x^{(2)} < t_4, x^{(1)} \geq t_3$}
\item{$R_5: x^{(1)} \geq t_1, x^{(2)} \geq t_4$}
\end{itemize}
What is the new decision tree and region distribution in the $x$-plane?
\begin{figure}[H]
\centering
\includegraphics[width=0.48\textwidth]{dtree7}
\hspace{1mm}
\includegraphics[width=0.48\textwidth]{dtree5}
\end{figure}
\end{example}
In theory, region can have any shape, but we use rectangles for simplicity and ease of interpretation.\\
\vspace{2mm}
The general procedure for generating decision trees and boundaries, in a \textbf{regression} setting, follow: 
\begin{enumerate}
\item{Divide the predictor space into $J$ distinct and nonoverlapping regions $R_1, ..., R_J$.}
\item{For every observation that falls into the same region, we make the same prediction$-$average the training observations in that region}
\item{Find $R_1, \cdot \cdot \cdot, R_J$ that minimize the residual sum of squared error (RSS)
$$\text{RSS} = \sum_{j = 1}^J \sum_{i \in R_j} \left(y_i - \overline{y}_{R_j} \right)^2 $$
where $\overline{y}_{R_j}$ is the mean response for the region $R_j$.}
\end{enumerate}
In more complex cases, it may not be feasible to check all possible combinations of regions. So, we may just optimize for one split. At the splitting step, pick a random predictor $X_j$ and a cutoff point $S$, and we split into two groups
$$R_1 ^{(j, s)} = \lbrace x | x_j < s \rbrace , \hspace{5mm} R_i^{(j, s)} = \lbrace x | x_j \geq s \rbrace$$
Then, we minimize
$$\sum_{i: x_i \in R_1 ^{(j, s)}} (y_i - \hat{y} R_i )^2 + \sum_{i: x_i \in R_2^{(j, s)}} (y_i - \hat{y}R_2 )^2$$
This is called \textit{recursive learning splitting}.\\
\vspace{2mm}
\textit{Remark}: For \textbf{classification} models, we take the majority vote in each region.\\
\vspace{2mm}
A decision tree function for regression models is computed by
$$f(x) = \sum_{m = 1}^J c_m \mathds{1}_{x \in R_m}$$
Here $\mathds{1}$ is the \textit{indicator function}, given by
$$\mathds{1}_{x \in R_m} = \begin{cases}
1 & \text{ if } x \in R_m\\
0 & \text{ otherwise }\\
\end{cases}$$
which really just tells us to return 1 if $x$ belongs to a particular region. $c_m$ is the average response in the corresponding region.\\
\begin{example}{}{dt3}
Suppose we wanted to train a decision tree regression model based on 5 data points
$$\left( \begin{pmatrix}
2\\
1\\
\end{pmatrix}, 5 \right),
\left( \begin{pmatrix}
3\\
2\\
\end{pmatrix}, 7 \right),
\left( \begin{pmatrix}
5\\
-2\\
\end{pmatrix}, -1 \right),
\left( \begin{pmatrix}
6\\
3\\
\end{pmatrix}, 2 \right), 
\left( \begin{pmatrix}
10\\
-5\\
\end{pmatrix}, -8 \right)$$
with the decision boundaries $D_1: x_1 < 4, D_2: x_1 < 6$. Construct a decision tree model $f(x)$ that predicts an outcome $y$ for new data. Additionally, compute the RSS.\\

Let $R_1$ be the region left of $D_1$, $R_2$ be the region between $D_2$ and $D_3$, and $R_3$ be the region to the right of $D_2$. Label $N_1 \to N_5$ be the given nodes from left to right. Then, $N_1, N_2 \in R_1, N_3, N_4 \in R_2$, and $N_5 \in R_3$. We compute $c_m$ as the average of the responses for each region.

$$c_1 = \frac{5 + 7}{2} = 6, \hspace{5mm} c_2 = \frac{2 - 1}{2} = \frac{1}{2}, \hspace{5mm} c_3 = \frac{-8}{1} = -8.$$
Then, the function can be modeled accordingly:
$$f(x) = 6 \cdot \mathds{1}_{x \in R_1} + \frac{1}{2} \cdot \mathds{1}_{x \in R_2} - 8 \cdot \mathds{1}_{x \in R_3}.$$
The RSS for each region is computed as the sum of differences between the observed value and average of observed values, squared.\\
For $R_1$:
$$\text{RSS}_1 = (5 - 6)^2 + (7 - 6)^2 = 2$$
For $R_2$:
$$\text{RSS}_2 = \left(-1 - \frac{1}{2} \right)^2 + \left(2 - \frac{1}{2} \right)^2 = 2.25 + 2.25 = 4.5$$
For $R_3$:
$$\text{RSS}_3 = (8 - 8)^2 = 0$$
The total RSS is therefore $\text{RSS}_1 + \text{RSS}_2 + \text{RSS}_3 = 2 + 4.5 + 0 = 6.5$.
\end{example}
Intuitively, if similar outputs are in the same region, we obtain a better cut.\\
\vspace{2mm}
Decision trees are more likely to overfit our training data, especially if we choose a greater depth (more decisions and nodes). In summary, they are sensitive to training data which leads to high variance and low bias. In a practical setting, decision boundaries for decision trees are nonlinear, which poses an advantage compared to a standard linear regression model. While we used linear boundaries in the previous examples, note that decision boundaries are typically \textbf{non-linear} and \textbf{non-parametric}.\\
\subsection{Random Forests}
The goal of this section is to produce a model that tries to solve the overfitting dilemma that decision trees are prone to.

\pagebreak
\section{Unsupervised Learning}
\end{flushleft}
\end{document}
